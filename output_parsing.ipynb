{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Decorators âœ¨ - output parsing\n",
    "\n",
    "In the notebook we will explore details of output parsing with decorators\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_decorators in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: langchain in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain_decorators) (0.0.194)\n",
      "Requirement already satisfied: promptwatch in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain_decorators) (0.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (2.0.15)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (0.5.7)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.6 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (0.0.6)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (1.24.3)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (1.10.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from langchain->langchain_decorators) (8.2.2)\n",
      "Requirement already satisfied: tiktoken in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from promptwatch->langchain_decorators) (0.4.0)\n",
      "Requirement already satisfied: tqdm in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from promptwatch->langchain_decorators) (4.65.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_decorators) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_decorators) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_decorators) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_decorators) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_decorators) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_decorators) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->langchain_decorators) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->langchain_decorators) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->langchain_decorators) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from pydantic<2,>=1->langchain->langchain_decorators) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from requests<3,>=2->langchain->langchain_decorators) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from requests<3,>=2->langchain->langchain_decorators) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from requests<3,>=2->langchain->langchain_decorators) (2023.5.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from tiktoken->promptwatch->langchain_decorators) (2023.6.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain->langchain_decorators) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/jurajbezdek/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->langchain_decorators) (1.0.0)\n",
      "env: OPENAI_API_KEY=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 #sk-********************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# install langchain_decorators (will install also langchain and promptwatch)\n",
    "!pip install langchain_decorators\n",
    "\n",
    "#########################################\n",
    "# you need to setup your openai api key\n",
    "#########################################\n",
    "%env OPENAI_API_KEY=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 #sk-******************************** \n",
    "\n",
    "\n",
    "import os\n",
    "if not os.environ[\"OPENAI_API_KEY\"]:\n",
    "    raise Exception(\"You need to setup your openai api key\")\n",
    "\n",
    "from langchain_decorators import GlobalSettings\n",
    "\n",
    "import logging\n",
    "\n",
    "# let's define our settings, just to make it more verbose for demonstration\n",
    "GlobalSettings.define_settings(\n",
    "    #default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally\n",
    "    #default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming\n",
    "    logging_level=logging.INFO, \n",
    "    print_prompt=True, \n",
    "    print_prompt_name=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic concepts\n",
    "\n",
    "- by default decorators are trying to determine the the expected format based the return type annotation.\n",
    "\n",
    "-  the basic rules are for `auto` output parsing follows these rules:\n",
    "  \n",
    "  - output type: `str` | `None` - > no output parsing is applied\n",
    "  - output type: `dict` -> `JsonOutputParser` will be used.\n",
    "    > will extract any JSON from the output. Only **one** JSON is expected in the output. The json can be buried in the code block, but it is not required. \n",
    "  - output type: `list` or `List[str]` -> `ListOutputParser`\n",
    "    > will parse **bullet lists** and **numbered lists** from the output (doesn't matter which one). This is tailored to the native behavior to most LLMs, that when asked for a list of something, they ususaly generate numbered or bullet list. Therefore you usually don't need to add any instructions when using this (works great with chatGPT).\n",
    "  - output type: subclass of `BaseClass` -> [PydanticOutputParser](#PydanticOutputParser)\n",
    "    > Expects JSON in that could be parsed as the desired BaseClass. Several tricks are used to make sure this \n",
    "\n",
    "- on the top ot this there are two more interesting parser available here:\n",
    "  - [CheckListParser](#CheckListParser) - similar to list, but expects items of the list in this pattern: `- {key}: {value}`\n",
    "  - [MarkdownStructureParser](#MarkdownStructureParser) - a powerful parser to extract complex structures with long texts. Among other things, it allows you to define specific format for specific sections. Great for combining long reasoning with tool and arguments!\n",
    "  \n",
    "- you can also use any of langchain output parser (`from langchain.output_parsers`) but by default, decorators are using customized output parsers available at `from langchain_decorators.output_parsers`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JsonOutputParser\n",
    "- will extract any JSON from the output. Only **one** JSON is expected in the output. The json can be buried in the code block, but it is not required. \n",
    "- returns `dict`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ListOutputParser\n",
    "- will parse **bullet lists** and **numbered lists** from the output (doesn't matter which one). This is tailored to the native behavior to most LLMs, that when asked for a list of something, they ususaly generate numbered or bullet list. Therefore you usually don't need to add any instructions when using this (works great with chatGPT).\n",
    "- returns `List[str]` \n",
    "\n",
    "## \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "> All the cells are designed to run on its own, except this initialization cell that is required to install the packages and setup override some default settings for more verbose logging for demonstration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install langchain_decorators (will install also langchain and promptwatch)\n",
    "!pip install langchain_decorators\n",
    "\n",
    "#########################################\n",
    "# you need to setup your openai api key\n",
    "#########################################\n",
    "%env OPENAI_API_KEY=sk-******************************** \n",
    "\n",
    "\n",
    "import os\n",
    "if not os.environ[\"OPENAI_API_KEY\"]:\n",
    "    raise Exception(\"You need to setup your openai api key\")\n",
    "\n",
    "from langchain_decorators import GlobalSettings\n",
    "\n",
    "import logging\n",
    "\n",
    "# let's define our settings, just to make it more verbose for demonstration\n",
    "GlobalSettings.define_settings(\n",
    "    #default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally\n",
    "    #default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming\n",
    "    logging_level=logging.INFO, \n",
    "    print_prompt=True, \n",
    "    print_prompt_name=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PromptDecoratorTemplate\noutput_parser\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m    Question: {question}\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m    {FORMAT_INSTRUCTIONS}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m give_me_straight_answer(topic\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mIs the earth flat?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/SourceCode/langchain-decorators/src/langchain_decorators/prompt_decorator.py:226\u001b[0m, in \u001b[0;36mllm_prompt.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    225\u001b[0m     print_log(log_object\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m> Entering \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m prompt decorator chain\u001b[39m\u001b[39m\"\u001b[39m, log_level\u001b[39m=\u001b[39mprompt_type\u001b[39m.\u001b[39mlog_level \u001b[39mif\u001b[39;00m prompt_type \u001b[39melse\u001b[39;00m logging\u001b[39m.\u001b[39mDEBUG,color\u001b[39m=\u001b[39mLogColors\u001b[39m.\u001b[39mWHITE_BOLD)\n\u001b[0;32m--> 226\u001b[0m     llmChain, chain_args \u001b[39m=\u001b[39m prepare_call_args(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    228\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m         result \u001b[39m=\u001b[39m llmChain\u001b[39m.\u001b[39mpredict(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mchain_args)\n",
      "File \u001b[0;32m~/SourceCode/langchain-decorators/src/langchain_decorators/prompt_decorator.py:163\u001b[0m, in \u001b[0;36mllm_prompt.<locals>.decorator.<locals>.prepare_call_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     memory\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m prompt_template \u001b[39m=\u001b[39m PromptDecoratorTemplate\u001b[39m.\u001b[39;49mfrom_func(func, \n\u001b[1;32m    164\u001b[0m                                                 template_format\u001b[39m=\u001b[39;49mtemplate_format, \n\u001b[1;32m    165\u001b[0m                                                 output_parser\u001b[39m=\u001b[39;49moutput_parser, \n\u001b[1;32m    166\u001b[0m                                                 format_instructions_parameter_key\u001b[39m=\u001b[39;49mformat_instructions_parameter_key,\n\u001b[1;32m    167\u001b[0m                                                 template_name\u001b[39m=\u001b[39;49mtemplate_name,\n\u001b[1;32m    168\u001b[0m                                                 template_version\u001b[39m=\u001b[39;49mtemplate_version,\n\u001b[1;32m    169\u001b[0m                                                 prompt_type\u001b[39m=\u001b[39;49mprompt_type,\n\u001b[1;32m    170\u001b[0m                                                 )\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m prompt_template\u001b[39m.\u001b[39mdefault_values:\n\u001b[1;32m    172\u001b[0m     kwargs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_template\u001b[39m.\u001b[39mdefault_values, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n",
      "File \u001b[0;32m~/SourceCode/langchain-decorators/src/langchain_decorators/prompt_template.py:268\u001b[0m, in \u001b[0;36mPromptDecoratorTemplate.from_func\u001b[0;34m(cls, func, template_name, template_version, output_parser, template_format, format_instructions_parameter_key, prompt_type)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported return type \u001b[39m\u001b[39m{\u001b[39;00mreturn_type\u001b[39m}\u001b[39;00m\u001b[39m for pydantic output parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m default_values \u001b[39m=\u001b[39m {k:v\u001b[39m.\u001b[39mdefault \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(func)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39mdefault\u001b[39m!=\u001b[39minspect\u001b[39m.\u001b[39mParameter\u001b[39m.\u001b[39mempty}\n\u001b[0;32m--> 268\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mbuild(\n\u001b[1;32m    269\u001b[0m     template_string\u001b[39m=\u001b[39;49mtemplate_string,\n\u001b[1;32m    270\u001b[0m     template_name\u001b[39m=\u001b[39;49mtemplate_name,\n\u001b[1;32m    271\u001b[0m     template_version\u001b[39m=\u001b[39;49mtemplate_version,\n\u001b[1;32m    272\u001b[0m     output_parser\u001b[39m=\u001b[39;49moutput_parser,\n\u001b[1;32m    273\u001b[0m     template_format\u001b[39m=\u001b[39;49mtemplate_format,\n\u001b[1;32m    274\u001b[0m     optional_variables\u001b[39m=\u001b[39;49m[\u001b[39m*\u001b[39;49mdefault_values\u001b[39m.\u001b[39;49mkeys()],\n\u001b[1;32m    275\u001b[0m     default_values\u001b[39m=\u001b[39;49mdefault_values,\n\u001b[1;32m    276\u001b[0m     format_instructions_parameter_key\u001b[39m=\u001b[39;49mformat_instructions_parameter_key,\n\u001b[1;32m    277\u001b[0m     prompt_type\u001b[39m=\u001b[39;49mprompt_type\n\u001b[1;32m    278\u001b[0m )\n",
      "File \u001b[0;32m~/SourceCode/langchain-decorators/src/langchain_decorators/prompt_template.py:186\u001b[0m, in \u001b[0;36mPromptDecoratorTemplate.build\u001b[0;34m(cls, template_string, template_name, template_format, output_parser, optional_variables, optional_variables_none_behavior, default_values, format_instructions_parameter_key, template_version, prompt_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m         input_variables\u001b[39m.\u001b[39mextend(message_template\u001b[39m.\u001b[39minput_variables)\n\u001b[1;32m    181\u001b[0m         prompt_template_drafts\u001b[39m.\u001b[39mappend(message_template)\n\u001b[0;32m--> 186\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    187\u001b[0m         input_variables\u001b[39m=\u001b[39;49minput_variables, \u001b[39m#defined in base\u001b[39;49;00m\n\u001b[1;32m    188\u001b[0m         output_parser\u001b[39m=\u001b[39;49moutput_parser,\u001b[39m#defined in base\u001b[39;49;00m\n\u001b[1;32m    189\u001b[0m         prompt_template_drafts\u001b[39m=\u001b[39;49mprompt_template_drafts,\n\u001b[1;32m    190\u001b[0m         template_name\u001b[39m=\u001b[39;49mtemplate_name,\n\u001b[1;32m    191\u001b[0m         template_version\u001b[39m=\u001b[39;49mtemplate_version,\n\u001b[1;32m    192\u001b[0m         template_string\u001b[39m=\u001b[39;49mtemplate_string,\n\u001b[1;32m    193\u001b[0m         template_format\u001b[39m=\u001b[39;49mtemplate_format,\n\u001b[1;32m    194\u001b[0m         optional_variables\u001b[39m=\u001b[39;49moptional_variables,\n\u001b[1;32m    195\u001b[0m         optional_variables_none_behavior\u001b[39m=\u001b[39;49moptional_variables_none_behavior,\n\u001b[1;32m    196\u001b[0m         default_values\u001b[39m=\u001b[39;49mdefault_values,\n\u001b[1;32m    197\u001b[0m         format_instructions_parameter_key\u001b[39m=\u001b[39;49mformat_instructions_parameter_key,\n\u001b[1;32m    198\u001b[0m         prompt_type\u001b[39m=\u001b[39;49mprompt_type\n\u001b[1;32m    199\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/langchain-decorators-cY9qGSig/lib/python3.9/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for PromptDecoratorTemplate\noutput_parser\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def give_me_straight_answer(question:str)->bool:\n",
    "    \"\"\"\n",
    "    Question: {question}\n",
    "    {FORMAT_INSTRUCTIONS}\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "give_me_straight_answer(topic=\"Is the earth flat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_decorators import llm_prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple script with async and streaming\n",
    "\n",
    "If we wan't to leverage streaming:\n",
    " - we need to define prompt as async function \n",
    " - turn on the streaming on the decorator, or we can define PromptType with streaming on\n",
    " - capture the stream using StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPrompt template name: write_me_short_post\u001b[0m\n",
      "\u001b[90mPrompt:\n",
      "Write me a short header for my post about Releasing a new App that can do real magic! for twitter platform. \n",
      "It should be for developers audience.\n",
      "(Max 15 words)\u001b[0m\n",
      "\u001b[90mâ€¢\u001b[0m\u001b[0m\"\u001b[0m\u001b[90mInt\u001b[0m\u001b[0mroducing\u001b[0m\u001b[90m the\u001b[0m\u001b[0m Revolutionary\u001b[0m\u001b[90m Twitter\u001b[0m\u001b[0m App\u001b[0m\u001b[90m for\u001b[0m\u001b[0m Developers\u001b[0m\u001b[90m:\u001b[0m\u001b[0m Real\u001b[0m\u001b[90m Magic\u001b[0m\u001b[0m at\u001b[0m\u001b[90m Your\u001b[0m\u001b[0m F\u001b[0m\u001b[90ming\u001b[0m\u001b[0mert\u001b[0m\u001b[90mips\u001b[0m\u001b[0m!\"\u001b[0m\u001b[90mâ€¢\u001b[0m\n",
      "Stream finished ... we can distinguish tokens thanks to alternating colors\n",
      "\n",
      "We've captured 21 tokensðŸŽ‰\n",
      "\n",
      "Here is the result:\n",
      "\"Introducing the Revolutionary Twitter App for Developers: Real Magic at Your Fingertips!\"\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import StreamingContext, llm_prompt\n",
    "\n",
    "@llm_prompt(capture_stream=True) # this will mark the prompt for streaming (usefull if we want stream just some prompts)\n",
    "async def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\n",
    "    \"\"\"\n",
    "    Write me a short header for my post about {topic} for {platform} platform. \n",
    "    It should be for {audience} audience.\n",
    "    (Max 15 words)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "async def run_prompt():\n",
    "    return await write_me_short_post(topic=\"Releasing a new App that can do real magic!\")\n",
    "\n",
    "tokens=[]\n",
    "def capture_stream_func(new_token:str):\n",
    "    tokens.append(new_token)\n",
    "\n",
    "\n",
    "with StreamingContext(stream_to_stdout=True, callback=capture_stream_func):\n",
    "    result = await run_prompt()\n",
    "    print(\"Stream finished ... we can distinguish tokens thanks to alternating colors\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nWe've captured\",len(tokens),\"tokensðŸŽ‰\\n\")\n",
    "print(\"Here is the result:\")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt declarations\n",
    "\n",
    " - with additional (non 'executable') documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note shat prompt is only the inside codeblcok\n",
      "\u001b[90mPrompt template name: write_me_short_post\u001b[0m\n",
      "\u001b[90mPrompt:\n",
      "Write me a short header for my post about Cookies for facebook platform. \n",
      "It should be for my mom audience.\n",
      "(Max 15 words)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Indulge in Sweet Treats: Easy Cookie Recipes for Busy Moms\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\n",
    "    \"\"\"\n",
    "    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.\n",
    "\n",
    "    It needs to be a code block, marked as a `<prompt>` language\n",
    "    ```<prompt>\n",
    "    Write me a short header for my post about {topic} for {platform} platform. \n",
    "    It should be for {audience} audience.\n",
    "    (Max 15 words)\n",
    "    ```\n",
    "\n",
    "    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\n",
    "    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"Note shat prompt is only the inside codeblock\")\n",
    "write_me_short_post(topic=\"Cookies\", platform=\"facebook\", audience=\"my mom\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt with messages\n",
    "\n",
    "We can use this technique to annotate also different ChatMessageTemplates ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPrompt template name: simulate_conversation\u001b[0m\n",
      "\u001b[90mPrompt:\n",
      "system: You are a a pirate hacker. You mus act like one.\n",
      "You reply always in code, using python or javascript code block...\n",
      "for example:\n",
      "\n",
      "... do not reply with anything else.. just with code - respecting your role.\n",
      "user: Helo, who are you\n",
      "assistant: ``` python\n",
      "def hello():\n",
      "    print(\"Argh... hello you pesky prirate\")\n",
      "```\n",
      "user: What is your purpose?\u001b[0m\n",
      "``` javascript\n",
      "const purpose = \"My purpose is to plunder and hack, arrrr!\";\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def simulate_conversation(human_input:str, agent_role:str=\"a pirate\"):\n",
    "    \"\"\"\n",
    "    ## System message\n",
    "     - note the `:system` sufix inside the <prompt:_role_> tag\n",
    "     \n",
    "\n",
    "    ```<prompt:system>\n",
    "    You are a {agent_role} hacker. You mus act like one.\n",
    "    You reply always in code, using python or javascript code block...\n",
    "    for example:\n",
    "    \n",
    "    ... do not reply with anything else.. just with code - respecting your role.\n",
    "    ```\n",
    "\n",
    "    # human message \n",
    "    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)\n",
    "    ``` <prompt:user>\n",
    "    Helo, who are you\n",
    "    ```\n",
    "    a reply:\n",
    "    \n",
    "\n",
    "    ``` <prompt:assistant>\n",
    "    \\``` python\n",
    "    def hello():\n",
    "        print(\"Argh... hello you pesky pirate\")\n",
    "    \\```\n",
    "    ```\n",
    "    - note the \\ escape chars inside the prompt to allow us to pass a code block example inside the prompt.\n",
    "\n",
    "\n",
    "    we can also add some history using placeholder\n",
    "    ```<prompt:placeholder>\n",
    "    {history}\n",
    "    ```\n",
    "    ```<prompt:user>\n",
    "    {human_input}\n",
    "    ```\n",
    "\n",
    "    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\n",
    "    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# the history is optional, ... the placeholder will just be ignored if not provided\n",
    "response =simulate_conversation(human_input=\"What is your purpose?\", history=None) \n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional sections\n",
    "- you can define a whole sections of your prompt that should be optional\n",
    "- if any input in the section is missing, the whole section wont be rendered\n",
    "\n",
    "the syntax for this is as follows:\n",
    "\n",
    "```\n",
    "  \"\"\"\n",
    "  this text will be rendered always, but\n",
    "\n",
    "  {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | \"\")   ?}\n",
    "\n",
    "  you can also place it in between the words\n",
    "  this too will be rendered{? , but\n",
    "    this  block will be rendered only if {this_value} and {this_value}\n",
    "    is not empty?} !\n",
    "  \"\"\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt with max words set to 10\n",
      "\u001b[90mPrompt template name: write_me_short_post\u001b[0m\n",
      "\u001b[90mPrompt:\n",
      "Write a  header about Cookies for facebook platform. \n",
      "It has to be 5 words long.\u001b[0m\n",
      "\"Craving Cookies? Learn More!\"\n",
      "\n",
      "\n",
      "Prompt with max words left as default=None\n",
      "\u001b[90mPrompt template name: write_me_short_post\u001b[0m\n",
      "\u001b[90mPrompt:\n",
      "Write a  header about Cookies for facebook platform.\u001b[0m\n",
      "\"Indulge in the Sweet World of Cookies on Facebook: Discover Delicious Recipes, Tips, and More!\"\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def write_me_short_post(topic:str, platform:str=\"twitter\",  max_words:int=None):\n",
    "    \"\"\" Write a  header about {topic} for {platform} platform. {?\n",
    "    It has to be {max_words} words long.\n",
    "    ?}\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# with the max words set to 10, the prompt will be:\n",
    "print(\"Prompt with max words set to 10\")\n",
    "print(write_me_short_post(topic=\"Cookies\", platform=\"facebook\", max_words=5))\n",
    "\n",
    "\n",
    "# without the max words, the part of the prompt wrapped in {? ?} will be ignored\n",
    "print(\"\\n\\nPrompt with max words left as default=None\")\n",
    "print(write_me_short_post(topic=\"Cookies\", platform=\"facebook\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output parsers\n",
    "- llm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string)\n",
    "- list, dict and pydantic outputs are also supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sweet Cravings',\n",
       " 'Cookie Haven',\n",
       " 'Crumbly Delights',\n",
       " 'Sugar Rush Cookies',\n",
       " 'The Cookie Jar Co.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def write_name_suggestions(company_business:str, count:int)->list:\n",
    "    \"\"\" Write me {count} good name suggestions for company that {company_business}\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "write_name_suggestions(company_business=\"sells cookies\", count=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic parser\n",
    "\n",
    "(note that by default we use different `pydantic` parser than the standard from LangChain. It has support for re-prompting llm to reformat the output and it generates shorter and more informative format instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "Result:\n",
      "{\n",
      "\"name\": \"Sweet Delights\",\n",
      "\"headline\": \"Indulge in our heavenly cookies!\",\n",
      "\"employees\": [\n",
      "    \"Emily Johnson - Head Baker\",\n",
      "    \"David Lee - Marketing Manager\",\n",
      "    \"Sarah Chen - Sales Representative\",\n",
      "    \"Michael Rodriguez - Operations Manager\",\n",
      "    \"Avery Thompson - Customer Service Representative\"\n",
      "    ]\n",
      "}\u001b[0m\n",
      "\n",
      "Company name:  Sweet Delights\n",
      "company headline:  Indulge in our heavenly cookies!\n",
      "company employees:  ['Emily Johnson - Head Baker', 'David Lee - Marketing Manager', 'Sarah Chen - Sales Representative', 'Michael Rodriguez - Operations Manager', 'Avery Thompson - Customer Service Representative']\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class TheOutputStructureWeExpect(BaseModel):\n",
    "    name:str = Field (description=\"The name of the company\")\n",
    "    headline:str = Field( description=\"The description of the company (for landing page)\")\n",
    "    employees:list[str] = Field(description=\"5-8 fake employee names with their positions\")\n",
    "\n",
    "@llm_prompt()\n",
    "def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:\n",
    "    \"\"\" Generate a fake company that {company_business}\n",
    "    {FORMAT_INSTRUCTIONS}\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "company = fake_company_generator(company_business=\"sells cookies\")\n",
    "\n",
    "# print the result nicely formatted\n",
    "print(\"Company name: \",company.name)\n",
    "print(\"company headline: \",company.headline)\n",
    "print(\"company employees: \",company.employees)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing parameters as object\n",
    "- you can also pass your inputs as a (non-kword) argument...\n",
    "\n",
    "###  Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPrompt template name: introduce_your_self\u001b[0m\n",
      "\u001b[90mPrompt:\n",
      "system: You are an assistant named John. \n",
      "Your role is to act as a pirate\n",
      "user: Introduce your self (in less than 20 words)\u001b[0m\n",
      "Ahoy mateys! I be John, yer trusty pirate assistant.\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "class AssistantPersonality(BaseModel):\n",
    "    assistant_name:str\n",
    "    assistant_role:str\n",
    "\n",
    "\n",
    "\n",
    "@llm_prompt\n",
    "def introduce_your_self(obj:AssistantPersonality)->str:\n",
    "    \"\"\"\n",
    "    ```Â <prompt:system>\n",
    "    You are an assistant named {assistant_name}. \n",
    "    Your role is to act as {assistant_role}\n",
    "    ```\n",
    "    ```<prompt:user>\n",
    "    Introduce your self (in less than 20 words)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "personality = AssistantPersonality(assistant_name=\"John\", assistant_role=\"a pirate\")\n",
    "\n",
    "print(introduce_your_self(personality))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex stuff\n",
    "\n",
    "What is even **more interesting use case** for this is to pack a bunch of functions into single object to share some inputs with multiple prompts and allowing us more Object-oriented approach\n",
    "\n",
    "Here is an example of complete ReAct reimplemented with lanchchain decoratorsâœ¨.\n",
    "\n",
    "*Hint: To make it async, just turn all the functions async* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Entering reason prompt decorator chain\u001b[0m\n",
      "\n",
      "\u001b[32mPrompt:\n",
      "system: You are an assistant that uses reasoning and tools to help user. You use tools for the task the tool is designed to. \n",
      "Before answering the question and/or using the tool, you should write down the explanation. \n",
      "\n",
      "Here is the list of tools available:\n",
      " - Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use this format:\n",
      "\n",
      "# Reasoning\n",
      "... write your reasoning here ...\n",
      "\n",
      "# Tool\n",
      "```json\n",
      "    {{\n",
      "        \"tool\": name of the tool to use,\n",
      "        \"tool_input\": the input for the tool\n",
      "    }}\n",
      "```\n",
      "\n",
      "# Observation\n",
      "output from the tool\n",
      "\n",
      "... repeat this # Reasoning, # Tool, # Observation sequence multiple times until you know the final answer, when you write:\n",
      "\n",
      "# Final answer\n",
      "... write the final answer \n",
      " in German here ...\n",
      "Make sure to write the final answer in in German!\n",
      "user: What is the surface of a sphere with radius with diameter of 100km?\u001b[0m\n",
      "\n",
      "\u001b[32m\n",
      "Result:\n",
      "# Reasoning\n",
      "The surface area of a sphere can be calculated using the formula:\n",
      "\n",
      "Surface Area = 4Ï€rÂ²\n",
      "\n",
      "where r is the radius of the sphere.\n",
      "\n",
      "We are given the diameter of the sphere, which is 100 km. We can use this to find the radius by dividing the diameter by 2.\n",
      "\n",
      "# Tool\n",
      "```json\n",
      "{\n",
      "    \"tool\": \"Calculator\",\n",
      "    \"tool_input\": \"4 * 3.14159 * (100/2)**2\"\n",
      "}\n",
      "```\n",
      "\n",
      "# \u001b[0m\n",
      "\n",
      "\u001b[1m> Entering reason prompt decorator chain\u001b[0m\n",
      "\n",
      "\u001b[32mPrompt:\n",
      "system: You are an assistant that uses reasoning and tools to help user. You use tools for the task the tool is designed to. \n",
      "Before answering the question and/or using the tool, you should write down the explanation. \n",
      "\n",
      "Here is the list of tools available:\n",
      " - Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use this format:\n",
      "\n",
      "# Reasoning\n",
      "... write your reasoning here ...\n",
      "\n",
      "# Tool\n",
      "```json\n",
      "    {{\n",
      "        \"tool\": name of the tool to use,\n",
      "        \"tool_input\": the input for the tool\n",
      "    }}\n",
      "```\n",
      "\n",
      "# Observation\n",
      "output from the tool\n",
      "\n",
      "... repeat this # Reasoning, # Tool, # Observation sequence multiple times until you know the final answer, when you write:\n",
      "\n",
      "# Final answer\n",
      "... write the final answer \n",
      " in German here ...\n",
      "Make sure to write the final answer in in German!\n",
      "user: What is the surface of a sphere with radius with diameter of 100km?\n",
      "assistant: # Tool\n",
      "```json\n",
      "{\"tool\": \"Calculator\", \"tool_input\": \"4 * 3.14159 * (100/2)**2\"}\n",
      "```\n",
      "# Observation\n",
      "\n",
      "Result from tool Calculator:\n",
      "\tAnswer: 31415.899999999998\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[32m\n",
      "Result:\n",
      "# Reasoning\n",
      "The surface area of a sphere can be calculated using the formula: 4 * pi * r^2, where r is the radius of the sphere. In this case, the diameter of the sphere is given as 100 km, so the radius is half of that, which is 50 km. We can substitute this value into the formula and use a calculator to find the surface area.\n",
      "\n",
      "# Final answer\n",
      "Die OberflÃ¤che einer Kugel mit einem Durchmesser von 100 km betrÃ¤gt etwa 31.415,9 kmÂ².\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Here is the agent's answer: Die OberflÃ¤che einer Kugel mit einem Durchmesser von 100 km betrÃ¤gt etwa 31.415,9 kmÂ².\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_decorators import llm_prompt\n",
    "from langchain.agents import load_tools\n",
    "from langchain.tools.base import BaseTool\n",
    "from textwrap import dedent\n",
    "from langchain_decorators import PromptTypes\n",
    "from langchain_decorators.output_parsers import JsonOutputParser\n",
    "import json\n",
    "\n",
    "tools = load_tools([ \"llm-math\"], llm=GlobalSettings.get_current_settings().default_llm)\n",
    "\n",
    "# you may, or may not use pydantic as your base class... totally up to you\n",
    "class MultilingualAgent:\n",
    "\n",
    "    def __init__(self,  tools:List[BaseTool],result_language:str=None) -> None:\n",
    "        self.tools = tools\n",
    "\n",
    "        # we can refer to our field in all out prompts\n",
    "        self.result_language = result_language\n",
    "        \n",
    "        self.agent_scratchpad = \"\" # we initialize our scratchpad\n",
    "        self.feedback = \"\" # we initialize our feedback if we get some error\n",
    "\n",
    "        # other settings\n",
    "        self.iterations=10\n",
    "        self.agent_format_instructions = dedent(\"\"\"\\\n",
    "            # Reasoning\n",
    "            ... write your reasoning here ...\n",
    "\n",
    "            # Tool\n",
    "            ```json\n",
    "                {{\n",
    "                    \"tool\": name of the tool to use,\n",
    "                    \"tool_input\": the input for the tool\n",
    "                }}\n",
    "            ```\n",
    "\n",
    "            # Observation\n",
    "            output from the tool\n",
    "\n",
    "            ... repeat this # Reasoning, # Tool, # Observation sequence multiple times until you know the final answer, when you write:\n",
    "\n",
    "            # Final answer\n",
    "            ... write the final answer \n",
    "            \"\"\")\n",
    "\n",
    "    @property\n",
    "    def tools_description(self)->str:  # we can refer to properties in out prompts too\n",
    "        return \"\\n\".join([f\" - {tool.name}: {tool.description}\" for tool in self.tools])\n",
    "\n",
    "    # we defined prompt type here, which will make \n",
    "    @llm_prompt(prompt_type=PromptTypes.AGENT_REASONING, output_parser=\"markdown\", stop_tokens=[\"Observation\"], verbose=True)\n",
    "    def reason(self)->dict:\n",
    "        \"\"\"\n",
    "        The system prompt:\n",
    "        ``` <prompt:system>\n",
    "        You are an assistant that uses reasoning and tools to help user. You use tools for the task the tool is designed to. \n",
    "        Before answering the question and/or using the tool, you should write down the explanation. \n",
    "        \n",
    "        Here is the list of tools available:\n",
    "        {tools_description}\n",
    "        \n",
    "        Use this format:\n",
    "        \n",
    "        {agent_format_instructions}{? in {result_language}?} here ...{?\n",
    "        Make sure to write the final answer in in {result_language}!?} \n",
    "        \n",
    "        ```\n",
    "        User question:\n",
    "        ```<prompt:user>\n",
    "        {question}\n",
    "        ```\n",
    "        Scratchpad:\n",
    "        ```<prompt:assistant>\n",
    "        {agent_scratchpad}\n",
    "        ```\n",
    "        ```<prompt:user>\n",
    "        {feedback}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def act(self, tool_name:str, tool_input:str)->str:\n",
    "        tool = next((tool for tool in self.tools if tool.name.lower()==tool_name.lower()==tool_name.lower()))\n",
    "        if tool is None:\n",
    "            self.feedback = f\"Tool {tool_name} is not available. Available tools are: {self.tools_description}\"\n",
    "            return\n",
    "        else:\n",
    "            try:\n",
    "                result = tool.run(tool_input)\n",
    "            except Exception as e:\n",
    "                if self.feedback is not None:\n",
    "                    # we've already experienced an error, so we are not going to try forever... let's raise this one\n",
    "                    raise e\n",
    "                self.feedback = f\"Tool {tool_name} failed with error: {e}.\\nLet's fix it and try again.\"\n",
    "            tool_instructions = json.dumps({\"tool\":tool.name, \"tool_input\":tool_input})\n",
    "            self.agent_scratchpad += f\"# Tool\\n```json\\n{tool_instructions}\\n```\\n# Observation\\n\\nResult from tool {tool_name}:\\n\\t{result}\\n\"\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, question):\n",
    "        for i in range(self.iterations):\n",
    "            reasoning = self.reason(question=question)\n",
    "            if reasoning.get(\"Final answer\") is not None:\n",
    "                return reasoning.get(\"Final answer\")\n",
    "            else:\n",
    "                tool_info = reasoning.get(\"Tool\")\n",
    "                tool_name, tool_input = (None, None)\n",
    "                if tool_info:\n",
    "                    tool_info_parsed = JsonOutputParser().parse(tool_info)\n",
    "                    tool_name = tool_info_parsed.get(\"tool\")\n",
    "                    tool_input = tool_info_parsed.get(\"tool_input\")\n",
    "\n",
    "                if tool_name is None or tool_input is None:\n",
    "                    self.feedback = \"Your response was not in the expected format. Please make sure to response in correct format:\\n\" + self.agent_format_instructions \n",
    "                    continue\n",
    "                self.act(tool_name, tool_input)\n",
    "        raise Exception(f\"Failed to answer the question after {self.iterations} iterations. Last result: {reasoning}\")\n",
    "\n",
    "        \n",
    "agent = MultilingualAgent(tools=tools, result_language=\"German\" )\n",
    "\n",
    "result = agent.run(\"What is the surface of a sphere with radius with diameter of 100km?\")\n",
    "\n",
    "print(\"\\n\\nHere is the agent's answer:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-decorators-cY9qGSig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
